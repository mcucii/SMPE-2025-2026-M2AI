
** Project Overview
This document presents the performance evaluation of a custom multi-threaded QuickSort algorithm executed on multi-core machines. The core objective is to measure and analyze the speedup gained by parallelization, using statistical comparison against both sequential and standard library sorting methods across defined input sizes.

** Organization
The project is organized into three primary directories:

*** /src
This directory contains the core C code for the parallel quicksort implementation being tested. The executable generated by the build process is expected to reside here.
In order to run core C code do the following:

Compilation:
#+BEGIN_SRC bash
make -C src/
#+END_SRC

Running the code:
#+BEGIN_SRC bash
./src/parallelQuicksort [ARRAY_SIZE]
#+END_SRC

*** /scripts
This directory contains the Jupyter notebook "quicksort_benchmarking.ipynb" which includes all experimental code, execution results, and visualizations.

*** /data
This directory serves as the output location for all generated results.
    - Extracted data (raw execution times) are stored in =quicksort_results.csv=.
    - The final performance visualization is stored in =quicksort_results.png=.

** Experiment Methodology
The performance evaluation follows a systematic approach:

1.  Metrics: Three execution times are captured for each run:
    - Sequential Quicksort Time
    - Parallel Quicksort Time
    - Built-in System Sort Time (for baseline comparison)

2.  Phased Performance Analysis:
The analysis was conducted in three distinct phases, progressively increasing the input size to accurately measure the crossover point where the parallel approach overcomes its threading overhead.

** Phase 1: Initial Analysis
Initial ad-hoc executions for small input size (default: 100,000), revealed that the Parallel Quicksort was consistently the slowest. The sequential implementation demonstrated the fastest performance. The parallel version was slower due to thread management overhead, which exceeded any potential benefits of parallel execution for these array sizes.

Ad-Hoc Run Examples:
#+BEGIN_SRC bash
./src/parallelQuicksort
Sequential quicksort took: 0.199948 sec.
Parallel quicksort took: 0.572463 sec.
Built-in quicksort took: 0.214211 sec.

./src/parallelQuicksort
Sequential quicksort took: 0.153966 sec.
Parallel quicksort took: 0.438146 sec.
Built-in quicksort took: 0.163230 sec.
#+END_SRC


** Phase 2: Low-Range Analysis ($100,000$ to $1,000,000$)

Now, I tested for ARRAY_SIZES ranging from $100000$ to $1000000$ with a step of $100000$, for $20$ repetitions, and the situation was following:

[[./data/masa_2025_15_10/quicksort_results1.png]]

We san see unexpected performance where the parallel implementation is consistently slower than both sequential and built-in sorting. The parallel version runs slower because managing and coordinating threads requires more time than what we save by dividing the work. Next, I will test with larger array sizes to see if the parallel approach becomes more effective.


** Phase 3: Crossover Point Identification (Extended Range: $1000,000$ to $10,000,000$)

To find the array size necessary to observe a speedup, the input range was dramatically increased (from $1,000,000$ to $10,000,000$ with $1,000,000$ step size). This test was executed also $20$ times. This preliminary test successfully identified the crossover point:
The Parallel Quicksort demonstrated superior performance, becoming the fastest method when the array size exceeded approximately $6,000,000$ elements.

[[./data/masa_2025_15_10/quicksort_resultsFINAL.png]]

The parallel implementation successfully outperforms both sequential and built-in sorting methods, demonstrating effective parallelization. The consistent performance advantage across all tested array sizes, with clear separation in confidence intervals, confirms the practical benefits of our parallel approach.
